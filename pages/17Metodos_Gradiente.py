import streamlit as st

st.markdown("<h1 style='text-align: center;'>M√©todos de Optimizaci√≥n Basados en Gradiente</h1>", unsafe_allow_html=True)

st.markdown("""
<div style='text-align: justify'>

Los m√©todos de optimizaci√≥n basados en gradiente son una clase fundamental de algoritmos utilizados para encontrar el m√≠nimo (o m√°ximo) de una funci√≥n. Estos m√©todos aprovechan la informaci√≥n de la **derivada** (o gradiente en funciones multivariables) para decidir en qu√© direcci√≥n avanzar con el fin de mejorar el valor de la funci√≥n objetivo.

A diferencia de los m√©todos directos, que no requieren derivadas y suelen evaluar la funci√≥n muchas veces para encontrar el m√≠nimo, los m√©todos basados en gradiente utilizan la pendiente de la funci√≥n para avanzar de forma m√°s inteligente hacia un √≥ptimo. Esto generalmente los hace **m√°s eficientes y r√°pidos**, especialmente en problemas de alta dimensi√≥n.

### ¬øC√≥mo funcionan?

El principio clave es el siguiente: el **gradiente** de una funci√≥n en un punto indica la direcci√≥n de mayor incremento. Por lo tanto, si queremos minimizar la funci√≥n, debemos movernos en la **direcci√≥n opuesta al gradiente**.

Uno de los algoritmos m√°s conocidos es el **descenso del gradiente**, donde se actualan los valores de las variables siguiendo esta regla:

<center>ùë•<sub>n+1</sub> = ùë•<sub>n</sub> ‚àí Œ± ¬∑ ‚àáf(ùë•<sub>n</sub>)</center>

Donde:

- ùë•<sub>n</sub> es el valor actual del punto,
- Œ± es la **tasa de aprendizaje** o paso de actualizaci√≥n,
- ‚àáf(ùë•<sub>n</sub>) es el **gradiente de la funci√≥n** en el punto actual.

### Ventajas

- Son **r√°pidos** cuando la funci√≥n es suave y diferenciable.
- Muy √∫tiles en problemas de gran escala, como en el entrenamiento de redes neuronales.
- Explotan la geometr√≠a de la funci√≥n para hacer pasos m√°s informados.

### Limitaciones

- Requieren que la funci√≥n sea **diferenciable**.
- No funcionan bien en funciones **discretas o con muchas discontinuidades**.
- Pueden **quedarse atrapados en m√≠nimos locales** si la funci√≥n no es convexa.
- Sensibles a la **elecci√≥n de la tasa de aprendizaje**.

### Aplicaciones t√≠picas

- Optimizaci√≥n en ingenier√≠a.
- Machine Learning y Deep Learning.
- Ajuste de par√°metros en modelos f√≠sicos y estad√≠sticos.
- Dise√±o √≥ptimo de sistemas.

### ¬øQu√© pasa si no hay derivadas?

En algunos casos, no se cuenta con la derivada anal√≠tica de la funci√≥n. Aun as√≠, es posible usar **derivadas num√©ricas** (como diferencias finitas) para aproximar el gradiente. Esto permite aplicar estos m√©todos incluso en contextos donde la funci√≥n no est√° expresada de forma expl√≠cita.

</div>
""", unsafe_allow_html=True)
